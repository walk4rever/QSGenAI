{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0ad07045",
   "metadata": {},
   "source": [
    "## Installation First"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7b96d1c",
   "metadata": {},
   "source": [
    "* This will install the bare minimum requirements of LangChain. A lot of the value of LangChain comes when integrating it with various model providers, datastores, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67663c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7374b016",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-community"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a59c5875",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-core"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bdbf0d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install \"langserve[all]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a575c4d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-cli"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0534be3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langsmith"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa4be146",
   "metadata": {},
   "source": [
    "## Run a local llama2 model with Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37a1af45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.llms import Ollama\n",
    "llm = Ollama(model=\"llama2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c67713d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.invoke(\"how can langsmith help with testing?\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bfc5e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(llm.invoke(\"hi this is rafael\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c245d843",
   "metadata": {},
   "source": [
    "## Prompt usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9c194d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"You are world class technical documentation writer.\"),\n",
    "    (\"user\", \"{input}\")\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05bd45d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "93688d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(chain.invoke({\"input\": \"how can langsmith help with testing?\"}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c44c8ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.output_parsers import StrOutputParser\n",
    "output_parser = StrOutputParser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03045283",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain = prompt | llm | output_parser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e47c116a",
   "metadata": {},
   "outputs": [],
   "source": [
    "chain.invoke({\"input\": \"how can langsmith help with testing?\"})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7658a692",
   "metadata": {},
   "source": [
    "## Retrieval mode"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e3da35b",
   "metadata": {},
   "source": [
    "* First, we need to load the data that we want to index. In order to do this, we will use the WebBaseLoader. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e4adc27",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install beautifulsoup4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeecc4bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import WebBaseLoader\n",
    "loader = WebBaseLoader(\"https://docs.smith.langchain.com\")\n",
    "\n",
    "docs = loader.load()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9a465a6",
   "metadata": {},
   "source": [
    "* Next, we need to index it into a vectorstore. This requires a few components, namely an embedding model and a vectorstore."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8592b56c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "\n",
    "embeddings = OllamaEmbeddings()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a4a61d7",
   "metadata": {},
   "source": [
    "* Now, we can use this embedding model to ingest documents into a vectorstore. We will use a simple local vectorstore, FAISS, for simplicity's sake."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d22ac1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install faiss-cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0f58dd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import FAISS\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter()\n",
    "documents = text_splitter.split_documents(docs)\n",
    "vector = FAISS.from_documents(documents, embeddings)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29c01d80",
   "metadata": {},
   "source": [
    "* Now that we have this data indexed in a vectorstore, we will create a retrieval chain. This chain will take an incoming question, look up relevant documents, then pass those documents along with the original question into an LLM and ask it to answer the original question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e016e23b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"Answer the following question based only on the provided context:\n",
    "<context>\n",
    "{context}\n",
    "</context>\n",
    "\n",
    "Question: {input}\n",
    "\"\"\")\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5763d53c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.documents import Document\n",
    "\n",
    "response = document_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"context\": [Document(page_content=\"langsmith can let you visualize test results\")]\n",
    "})\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9550849d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_retrieval_chain\n",
    "\n",
    "retriever = vector.as_retriever()\n",
    "retrieval_chain = create_retrieval_chain(retriever, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6594ed5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = retrieval_chain.invoke({\"input\": \"how can langsmith help with testing?\"})\n",
    "print(response[\"answer\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d92d438f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.chains import create_history_aware_retriever\n",
    "from langchain_core.prompts import MessagesPlaceholder\n",
    "\n",
    "# First we need a prompt that we can pass into an LLM to generate this search query\n",
    "\n",
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "    (\"user\", \"Given the above conversation, generate a search query to look up in order to get information relevant to the conversation\") \n",
    "])\n",
    "\n",
    "retriever_chain = create_history_aware_retriever(llm, retriever, prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f323559d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_core.messages import HumanMessage, AIMessage\n",
    "\n",
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"), AIMessage(content=\"Yes!\")]\n",
    "\n",
    "retriever_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad905c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"Answer the user's question based on the below context:\\n\\n{context}\"),\n",
    "    MessagesPlaceholder(variable_name=\"chat_history\"),\n",
    "    (\"user\", \"{input}\"),\n",
    "])\n",
    "\n",
    "document_chain = create_stuff_documents_chain(llm, prompt)\n",
    "\n",
    "retriever_chain = create_retrieval_chain(retriever_chain, document_chain)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff93d3ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "chat_history = [HumanMessage(content=\"Can LangSmith help test my LLM applications?\"),\n",
    "               AIMessage(content=\"Yes!\")]\n",
    "\n",
    "retrieval_chain.invoke({\n",
    "    \"chat_history\": chat_history,\n",
    "    \"input\": \"Tell me how\"\n",
    "})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "669e2f1b",
   "metadata": {},
   "source": [
    "## Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fbe9af5",
   "metadata": {},
   "source": [
    "* The following only work with OpenAI, I will verify it now. (Feb 20, 2024.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c70e178",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.tools.retriever import create_retriever_tool\n",
    "\n",
    "retriever_tool = create_retriever_tool(\n",
    "    retriever,\n",
    "    \"langsmith_search\",\n",
    "    \"Search for information about LangSmith. For any questions about LangSmith, you must use this tool!\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0350dbb",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install langchain-openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7550448f",
   "metadata": {},
   "source": [
    "## Serving with LangServe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "073bc26b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"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\"\n",
    "\n",
    "! env | grep API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef821ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Please check the serve.py code for server side\n",
    "\n",
    "!python serve.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdfb5fe2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# client side code to invoke\n",
    "\n",
    "from langserve import RemoteRunnable\n",
    "\n",
    "remote_chain = RemoteRunnable(\"http://localhost:8000/agent/\")\n",
    "\n",
    "remote_chain.invoke({\n",
    "    \"input\": \"how can langsmith help with testing?\",\n",
    "    \"chat_history\": []\n",
    "})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
