{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "8556956c",
      "metadata": {
        "id": "8556956c"
      },
      "source": [
        "# Chapter 13. Computational Performance\n",
        "\n",
        "* This chapter will focus on the major factors that affect computational performance: imperative programming, symbolic programming, asynchronous computing, automatic parallelism, and multi-GPU computation."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fe71ac0f",
      "metadata": {
        "id": "fe71ac0f"
      },
      "source": [
        "## Chapter 13.1 Compilers and Interpreters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "9641412b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9641412b",
        "outputId": "2242d029-b842-4e2d-cc94-848d8911ae2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10\n"
          ]
        }
      ],
      "source": [
        "def add(a, b):\n",
        "    return a + b\n",
        "\n",
        "def fancy_func(a, b, c, d):\n",
        "    e = add(a, b)\n",
        "    f = add(c, d)\n",
        "    g = add(e, f)\n",
        "\n",
        "    return g\n",
        "\n",
        "print(fancy_func(1, 2, 3, 4))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "e6964dc0",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e6964dc0",
        "outputId": "2acbb8bb-52fc-4de4-d582-8f6779d08344"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0410, 0.2545]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 7
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n",
        "\n",
        "# Factory for networks\n",
        "def get_net():\n",
        "    net = nn.Sequential(nn.Linear(512, 256),\n",
        "                       nn.ReLU(),\n",
        "                       nn.Linear(256, 128),\n",
        "                       nn.ReLU(),\n",
        "                       nn.Linear(128, 2))\n",
        "    return net\n",
        "\n",
        "x = torch.randn(size=(1, 512))\n",
        "net = get_net()\n",
        "net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "3e6ffe0d",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3e6ffe0d",
        "outputId": "27c4fbb8-4f02-469e-aeca-840d181c3424"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[0.0410, 0.2545]], grad_fn=<AddmmBackward0>)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ],
      "source": [
        "# By converting the model using torch.jit.script function,\n",
        "# we are able to compile and optimize the computation in the MLP.\n",
        "\n",
        "net = torch.jit.script(net)\n",
        "net(x)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0334bd6d",
      "metadata": {
        "id": "0334bd6d"
      },
      "outputs": [],
      "source": [
        "# Benchmark to compare with or without jitscript\n",
        "\n",
        "class Benchmark:\n",
        "    \"\"\"For measuring running time.\"\"\"\n",
        "\n",
        "    def __init__(self, description='Done'):\n",
        "        self.description = description\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.timer = d2l.Timer()\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args):\n",
        "        print(f'{self.description}: {self.timer.stop():.4f} sec')\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "a0ab2644",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a0ab2644",
        "outputId": "a0989a90-7cec-4821-dfc6-733b1c0fd5bb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Without torchscript: 0.2719 sec\n",
            "With torchscript: 0.2697 sec\n"
          ]
        }
      ],
      "source": [
        "net = get_net()\n",
        "\n",
        "with Benchmark('Without torchscript'):\n",
        "    for i in range(3000):\n",
        "        net(x)\n",
        "\n",
        "net = torch.jit.script(net)\n",
        "\n",
        "with Benchmark('With torchscript'):\n",
        "    for i in range(3000):\n",
        "        net(x)\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "be2ab9f9",
      "metadata": {
        "id": "be2ab9f9",
        "outputId": "2a9c0ed8-55f9-4cd6-8209-6e710aba28c3"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "-rw-r--r--  1 yfzhu  staff   651K Jan 29 11:47 my_mlp\r\n"
          ]
        }
      ],
      "source": [
        "net.save('my_mlp')\n",
        "!ls -lh my_mlp*"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "95eee2b0",
      "metadata": {
        "id": "95eee2b0"
      },
      "source": [
        "## Chapter 13.2 Asynchronous Computation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aa13e9ae",
      "metadata": {
        "id": "aa13e9ae"
      },
      "outputs": [],
      "source": [
        "# Installation of d2l packages\n",
        "\n",
        "!pip install torch==2.0.0 torchvision==0.15.1\n",
        "!pip install d2l==1.0.3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "84668dcc",
      "metadata": {
        "id": "84668dcc"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import subprocess\n",
        "import numpy\n",
        "import torch\n",
        "from torch import nn\n",
        "from d2l import torch as d2l\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "74f4f88b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "74f4f88b",
        "outputId": "cdea75e0-0fa0-4ea3-8f46-2e6705b9f1e4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "numpy: 1.4007 sec\n",
            "torch: 0.0085 sec\n"
          ]
        }
      ],
      "source": [
        "# Warmup for GPU computation\n",
        "device = d2l.try_gpu()\n",
        "a = torch.randn(size=(1000, 1000), device=device)\n",
        "b = torch.mm(a, a)\n",
        "\n",
        "with d2l.Benchmark('numpy'):\n",
        "    for _ in range(10):\n",
        "        a = numpy.random.normal(size=(1000, 1000))\n",
        "        b = numpy.dot(a, a)\n",
        "\n",
        "with d2l.Benchmark('torch'):\n",
        "    for _ in range(10):\n",
        "        a = torch.randn(size=(1000, 1000), device=device)\n",
        "        b = torch.mm(a, a)\n",
        "    torch.cuda.synchronize(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "d400ea30",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d400ea30",
        "outputId": "8cc06e71-5656-4dd3-89ca-c1be4fee01ca"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "tensor([[3., 3.]], device='cuda:0')"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ],
      "source": [
        "# Another example  to understand the dependency graph.\n",
        "\n",
        "x = torch.ones((1, 2), device=device)\n",
        "y = torch.ones((1, 2), device=device)\n",
        "z = x * y + 2\n",
        "z"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 13.3 Automatci Parallelism\n"
      ],
      "metadata": {
        "id": "Poi8DZCjfgmr"
      },
      "id": "Poi8DZCjfgmr"
    },
    {
      "cell_type": "code",
      "source": [
        "devices = d2l.try_all_gpus()\n",
        "\n",
        "def run(x):\n",
        "  return [x.mm(x) for _ in range(50)]\n",
        "\n",
        "x_gpu1 = torch.rand(size=(4000, 4000), device=devices[0])\n",
        "x_gpu2 = torch.rand(size=(4000, 4000), device=devices[1])"
      ],
      "metadata": {
        "id": "0rXhT9eZffFg"
      },
      "id": "0rXhT9eZffFg",
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "run(x_gpu1)\n",
        "run(x_gpu2)\n",
        "\n",
        "torch.cuda.synchronize(devices[0])\n",
        "torch.cuda.synchronize(devices[1])\n",
        "\n",
        "with d2l.Benchmark('GPU 1 time'):\n",
        "  run(x_gpu1)\n",
        "  torch.cuda.synchronize(devices[0])\n",
        "\n",
        "with d2l.Benchmark('GPU 2 time'):\n",
        "  run(x_gpu2)\n",
        "  torch.cuda.synchronize(devices[1])\n",
        "\n"
      ],
      "metadata": {
        "id": "0zTB7ILlgKpd",
        "outputId": "b50df5ed-5438-474b-9b3f-190c083ebf86",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "0zTB7ILlgKpd",
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 1 time: 1.9003 sec\n",
            "GPU 2 time: 1.9108 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with d2l.Benchmark('GPU1 & GPU2'):\n",
        "  run(x_gpu1)\n",
        "  run(x_gpu2)\n",
        "  torch.cuda.synchronize()\n",
        "\n"
      ],
      "metadata": {
        "id": "CaQqwNfzg9iN",
        "outputId": "18880a01-a01d-4fc9-88cf-7c7e748be7e7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "CaQqwNfzg9iN",
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU1 & GPU2: 3.6658 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def copy_to_cpu(x, non_blocking=False):\n",
        "  return [y.to('cpu', non_blocking=non_blocking) for y in x]\n",
        "\n",
        "with d2l.Benchmark('Run on GPU1'):\n",
        "  y = run(x_gpu1)\n",
        "  torch.cuda.synchronize()\n",
        "\n",
        "with d2l.Benchmark('Run on CPU'):\n",
        "  y_cpu = copy_to_cpu(y)\n",
        "  torch.cuda.synchronize()"
      ],
      "metadata": {
        "id": "TyyrzVK5hNCo",
        "outputId": "187a7207-8707-4a4f-b20d-ec253105c50a",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "TyyrzVK5hNCo",
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run on GPU1: 1.9252 sec\n",
            "Run on CPU: 3.2244 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with d2l.Benchmark('Run on GPU1 and copy to CPU'):\n",
        "  y = run(x_gpu1)\n",
        "  y_cpu = copy_to_cpu(y, True)\n",
        "  torch.cuda.synchronize()"
      ],
      "metadata": {
        "id": "V4hRcF9KiCWm",
        "outputId": "67b990fe-6979-4185-92d8-5346d9ed826d",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "V4hRcF9KiCWm",
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Run on GPU1 and copy to CPU: 2.6150 sec\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Chapter 13.4 Hardware"
      ],
      "metadata": {
        "id": "K4Jt4lMYih9z"
      },
      "id": "K4Jt4lMYih9z"
    },
    {
      "cell_type": "code",
      "source": [
        "!nvidia-smi"
      ],
      "metadata": {
        "id": "iPQ3hojTiTvJ",
        "outputId": "c4d2865e-bab6-4436-82a0-c99282bc121b",
        "colab": {
          "base_uri": "https://localhost:8080/"
        }
      },
      "id": "iPQ3hojTiTvJ",
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mon Jan 29 04:48:12 2024       \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| NVIDIA-SMI 535.104.05             Driver Version: 535.104.05   CUDA Version: 12.2     |\n",
            "|-----------------------------------------+----------------------+----------------------+\n",
            "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
            "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
            "|                                         |                      |               MIG M. |\n",
            "|=========================================+======================+======================|\n",
            "|   0  Tesla T4                       Off | 00000000:00:04.0 Off |                    0 |\n",
            "| N/A   77C    P0              35W /  70W |   6503MiB / 15360MiB |      0%      Default |\n",
            "|                                         |                      |                  N/A |\n",
            "+-----------------------------------------+----------------------+----------------------+\n",
            "                                                                                         \n",
            "+---------------------------------------------------------------------------------------+\n",
            "| Processes:                                                                            |\n",
            "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
            "|        ID   ID                                                             Usage      |\n",
            "|=======================================================================================|\n",
            "+---------------------------------------------------------------------------------------+\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.18"
    },
    "colab": {
      "provenance": []
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}