{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efe58edd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify whether HF transformers are installed well\n",
    "\n",
    "from transformers import pipeline\n",
    "print(pipeline('sentiment-analysis')('we love you really so much'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfd6b1d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try to transcribe a audio file through pipeline\n",
    "\n",
    "# And the default is to use the facebook's Wav2Vec2\n",
    "\n",
    "transcriber = pipeline(task=\"automatic-speech-recognition\")\n",
    "\n",
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad99cff2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenAI's Whisper large-v2 model\n",
    "\n",
    "transcriber = pipeline(model=\"openai/whisper-large-v2\")\n",
    "\n",
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c967cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use OpenAI's Whisper large-v2 model, but with more inputs\n",
    "\n",
    "transcriber = pipeline(model=\"openai/whisper-large-v2\")\n",
    "\n",
    "transcriber(\n",
    "    [\n",
    "        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\",\n",
    "        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2be848b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install Accelerate package for device map setting\n",
    "\n",
    "! pip install --upgrade accelerate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf3c2b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transcribe the audio files with device_map parameter setup, not sure if it works\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "transcriber = pipeline(model=\"openai/whisper-large-v2\", device_map=\"auto\")\n",
    "\n",
    "transcriber(\n",
    "    [\n",
    "        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\",\n",
    "        \"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/1.flac\",\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a8c393",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup with batch_size\n",
    "\n",
    "transcriber = pipeline(model=\"openai/whisper-large-v2\", batch_size=2)\n",
    "\n",
    "audio_filenames = [f\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/{i}.flac\" for i in range(1, 5)]\n",
    "\n",
    "texts = transcriber(audio_filenames)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c747acd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Task specific parameter like timestamps\n",
    "\n",
    "transcriber = pipeline(model=\"openai/whisper-large-v2\", return_timestamps=True)\n",
    "\n",
    "transcriber(\"https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307384b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using datasets\n",
    "\n",
    "# KeyDataset is a util that will just output the item we're interested in.\n",
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from datasets import load_dataset\n",
    "\n",
    "pipe = pipeline(model=\"hf-internal-testing/tiny-random-wav2vec2\", device=0)\n",
    "dataset = load_dataset(\"hf-internal-testing/librispeech_asr_dummy\", \"clean\", split=\"validation[:10]\")\n",
    "\n",
    "for out in pipe(KeyDataset(dataset, \"audio\")):\n",
    "    print(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04f251f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install soundfile module\n",
    "\n",
    "! pip install --upgrade soundfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ef71c46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try an image pipeline\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "vision_classifier = pipeline(model=\"google/vit-base-patch16-224\")\n",
    "\n",
    "preds = vision_classifier(\n",
    "    images=\"https://huggingface.co/datasets/huggingface/documentation-images/resolve/main/pipeline-cat-chonk.jpeg\"\n",
    ")\n",
    "\n",
    "preds = [{\"score\": round(pred[\"score\"], 4), \"label\": pred[\"label\"]} for pred in preds]\n",
    "\n",
    "preds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bb9b302f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sequence': 'I have a problem with my iphone that needs to be resolved asap!!',\n",
       " 'labels': ['urgent', 'phone', 'computer', 'not urgent', 'tablet'],\n",
       " 'scores': [0.5036354660987854,\n",
       "  0.47879987955093384,\n",
       "  0.012600123882293701,\n",
       "  0.0026557897217571735,\n",
       "  0.0023087516892701387]}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Try some NLP tasks\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "# This model is a `zero-shot-classification` model.\n",
    "# It will classify text, except you are free to choose any label you might imagine\n",
    "\n",
    "classifier = pipeline(model=\"facebook/bart-large-mnli\")\n",
    "\n",
    "classifier(\n",
    "    \"I have a problem with my iphone that needs to be resolved asap!!\",\n",
    "    candidate_labels=[\"urgent\", \"not urgent\", \"phone\", \"tablet\", \"computer\"],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61f0394e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install pytesseract first to run the multimodal examples\n",
    "\n",
    "! sudo apt install -y tesseract-ocr\n",
    "! pip install pytesseract\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17431bce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Try a multi-modal cass, ask a question for an image\n",
    "\n",
    "from transformers import pipeline\n",
    "\n",
    "vqa = pipeline(model=\"impira/layoutlm-document-qa\")\n",
    "\n",
    "vqa(\n",
    "    image=\"https://huggingface.co/spaces/impira/docquery/resolve/2359223c1837a7587402bda0f2643382a6eefeab/invoice.png\",\n",
    "    question=\"What is the invoice number?\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bafbf712",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install accelerate\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "\n",
    "pipe = pipeline(model=\"facebook/opt-1.3b\", torch_dtype=torch.bfloat16)\n",
    "\n",
    "output = pipe(\"This is a cool example!\", do_sample=True, top_p=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fc01cca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_pytorch_p310",
   "language": "python",
   "name": "conda_pytorch_p310"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
